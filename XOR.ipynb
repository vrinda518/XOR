{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfRovZMXLQr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWGQZa3LYbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xor = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "basvRd_hUcv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(X):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      X[i][j] = max(X[i][j],0)\n",
        "  return X\n",
        "def d_relu(X):\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      if X[i][j]>0:\n",
        "        X[i][j]=1\n",
        "      else:\n",
        "        X[i][j]=0\n",
        "  return X\n"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x0H9zGCU8of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "  sum_denom = np.sum(X,axis = 1)\n",
        "  for i in range(X.shape[0]):\n",
        "        X[i, :] = X[i, :] / sum_denom[i]\n",
        "  return X"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnof8yGMRnTa",
        "colab_type": "text"
      },
      "source": [
        "INITIALIZE WEIGHTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAddWNg-O7w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(n,h,o):\n",
        "  parameters = {}\n",
        "  parameters[\"W1\"] = np.random.randn(h,n)*0.001\n",
        "  parameters[\"b1\"] = np.zeros((h,1))\n",
        "  parameters[\"W2\"] = np.random.randn(o,h)*0.001\n",
        "  parameters[\"b2\"] = np.zeros((o,1))\n",
        "  return parameters"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdQGF3goR3WX",
        "colab_type": "text"
      },
      "source": [
        "FORWARD PROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsR0A_wXRmN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(X,n,h,o,parameters):\n",
        "  ##parameters = initialize_weights(n,h,o)\n",
        "  W1 = parameters[\"W1\"] \n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"] \n",
        "  b2 = parameters[\"b2\"]\n",
        "  cache = {}\n",
        "\n",
        "  cache[\"Z1\"] = W1.dot(X) + b1\n",
        "  cache[\"A1\"] = relu(cache[\"Z1\"]) \n",
        "  cache[\"Z2\"] = W2.dot(cache[\"A1\"]) + b2\n",
        "  cache[\"A2\"] = softmax(cache[\"Z2\"])\n",
        "  y_pred = cache[\"A2\"]\n",
        "  return cache, y_pred\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lvc1GG1mNwd",
        "colab_type": "text"
      },
      "source": [
        "LOSS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXLdaz4wX25n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y,y_pred):\n",
        "  loss = 0\n",
        "  m = y.shape[0]\n",
        "  for i in range(m):\n",
        "    loss += -np.log(y_pred[i][y[i]])\n",
        "    loss = loss/m\n",
        "  return loss"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZaLfprUXJsM",
        "colab_type": "text"
      },
      "source": [
        "BACKWARD PROPAGATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0S3hncFXJZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_prop(X,y,n,h,o,parameters,cache,y_pred): \n",
        "  ny = len(np.unique(y))\n",
        "  y1 = keras.utils.to_categorical(y[:],num_classes = ny)\n",
        "  Z1 = cache[\"Z1\"]\n",
        "  A1 = cache[\"A1\"]  \n",
        "  Z2 = cache[\"Z2\"]\n",
        "  A2 = cache[\"A2\"] \n",
        "  m = X.shape[0]\n",
        "  grads = {}\n",
        "  dz2 = y_pred-y1\n",
        "  grads[\"dw2\"] = (1/m)*(dz2.dot(A1.T))\n",
        "  grads[\"db2\"] = (1/m)*np.sum(dz2,axis=1,keepdims=True)\n",
        "  dz1 = ((parameters[\"W2\"].T).dot(dz2))*d_relu(Z1)\n",
        "  grads[\"dw1\"] = (1/m)*(dz1.dot(X.T))\n",
        "  grads[\"db1\"] = (1/m)*np.sum(dz1,axis=1,keepdims=True)\n",
        "  return grads\n",
        "  \n",
        "\n"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiWPU1l8Vk5A",
        "colab_type": "text"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SIU7qW6mRck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(X,y,h,eta=0.01,n=1000):\n",
        "  n = X.shape[0]\n",
        "  o = y.shape[0]\n",
        "  params = initialize_weights(n,h,o)\n",
        "  for i in range(n):\n",
        "    \n",
        "    cache,y_pred = forward_prop(X,n,h,o,params)\n",
        "    grads = back_prop(X,y,n,h,o,params,cache,y_pred)\n",
        "    params[\"W1\"] -= eta*grads[\"dw1\"]\n",
        "    params[\"b1\"] -= eta*grads[\"db1\"]\n",
        "    params[\"W2\"] -= eta*grads[\"dw2\"]\n",
        "    params[\"b2\"] -= eta*grads[\"db2\"]\n",
        "    \n",
        "  return params\n",
        "\n"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmooF4cg_H-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X,params,y):\n",
        "  W1 = params[\"W1\"] \n",
        "  b1 = params[\"b1\"]\n",
        "  W2 = params[\"W2\"] \n",
        "  b2 = params[\"b2\"]\n",
        "  Z1 = W1.dot(X) + b1\n",
        "  A1 = relu(Z1) \n",
        "  Z2 = W2.dot(A1) + b2\n",
        "  A2 = softmax(Z2)\n",
        "  print(loss(y,A2))\n",
        "  y_pred = np.argmax(A2,axis=1)\n",
        "  return y_pred"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4xthTYnxZHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c6e008f5-fadb-4a27-d58e-2b3b13960213"
      },
      "source": [
        "params = training(xor,y,512,0.006,20000)\n",
        "print(predict(xor,params,y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1972466]\n",
            "[0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}